{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Webscraping: How to make sure you don't get blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims:\n",
    "\n",
    "- Write scripts that can handle errors and minimize the likelihood of your IP address getting blocked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Talk about the legality of scraping\n",
    "- Practice scraping\n",
    "- Look at ways to programmatically avoid getting banned\n",
    "- Set up the selenium webdriver\n",
    "- Learn how to use Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check 200 status code\n",
    "It is always good to check the HTTP status code earlier and proceed accordingly.\n",
    "\n",
    "This is good:\n",
    "\n",
    "~~~\n",
    "if response.status_code == 200:\n",
    "   #Proceed further\n",
    "~~~\n",
    "\n",
    "This is better:\n",
    "\n",
    "~~~~\n",
    "if response.status_code != 200:\n",
    "  return False\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "    # include code to do status check\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print( page.status_code)\n",
    "    \n",
    "    # more code to process the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Never Trust HTML\n",
    "\n",
    "Especially if you can’t control it. Web scraping depends on HTML DOM, a simple change in element or class name could break your entire script. The best way to deal with it is to check if it returns `None`.\n",
    "\n",
    "~~~\n",
    "page_count = soup.select('.pager-pages > li > a')\n",
    "if page_count:\n",
    " #do your stuff\n",
    "else:\n",
    " # ALERT!! Send notification to Admin\n",
    "~~~\n",
    "\n",
    "Here I am checking whether the CSS selector returned something legitimate, if yes then proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_links= []\n",
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print( page.status_code)\n",
    "        broken_links.append(url)\n",
    "        continue\n",
    "    \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 .  Set headers\n",
    "\n",
    "`requests` does not force you to use request headers while sending requests, but there are few smart websites that do not let you to get read anything important unless certain headers are not set in it. Once I faced the situation that the HTML I was seeing in browser was different than what I was getting via my script, kind of like magic huh. So, it is always good to make your requests as legitimate as you can. The least you should do is to set a User-Agent.\n",
    "\n",
    "~~~\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url, headers=headers, timeout=5)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print(page.status_code)\n",
    "    \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set timeout\n",
    "\n",
    "One of the issues with `requests` is that, if you don’t mention **timeout**, it will continue waiting for a response indefinitely. If your request is never fulfilled, it will leave your script haning there waiting for a response.  \n",
    "\n",
    "To set the request’s timeout, use the timeout parameter. timeout can be an integer or float representing the number of seconds to wait on a response before timing out:\n",
    "\n",
    "~~~\n",
    "response = requests.get(url, headers=headers, timeout=5)\n",
    "~~~\n",
    "\n",
    "\n",
    "You can also pass a tuple to timeout with the first element being a connect timeout (the time it allows for the client to establish a connection to the server), and the second being a read timeout (the time it will wait on a response once your client has established a connection):\n",
    "\n",
    "~~~ \n",
    "requests.get('https://api.github.com', timeout=(2, 5))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers, timeout=(2,5))\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print(page.status_code)\n",
    "    \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exception handling\n",
    "\n",
    "It is always good to implement exception handling. It does not only help to avoid unexpected exit of script but can also help to log errors and info notification. When using Python requests I prefer to catch exceptions like this:\n",
    "\n",
    "~~~\n",
    "try:\n",
    "    # your logic is here\n",
    "\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "    print(str(e))\n",
    "except requests.Timeout as e:\n",
    "    print(\"OOPS!! Timeout Error\")\n",
    "    print(str(e))\n",
    "except requests.RequestException as e:\n",
    "    print(\"OOPS!! General Error\")\n",
    "    print(str(e))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Someone closed the program\") \n",
    "~~~\n",
    "\n",
    "Check the very last one. This one tells the program that if someone wants to terminate program by using Ctrl+C then it wrap things up first and then exist. This situation is good if you are storing information in file and wants to dump all at the time of exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            print(page.status_code)\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "        \n",
    "        \n",
    "        \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is starting to get long and hard to read. So let's start to modularize it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            print(page.status_code)\n",
    "\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "        \n",
    "        \n",
    "    return page\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace a chunk of our code with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    #use our new function to process each url\n",
    "    page = get_page(url)\n",
    "        \n",
    "        \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regulate your request pace\n",
    "\n",
    "Many websites have a limit on how many times you can ping a website within a minute/hour/day. YOu want to be aware of that and change your script in order to account for that.\n",
    "\n",
    "One example is using the `sleep()` function that is a part of the time package.  This can pause your script for a set amount of time.\n",
    "\n",
    "~~~\n",
    "import time\n",
    " \n",
    " \n",
    "## Star loop ##\n",
    "for url in urls:\n",
    "\n",
    "    # try to make resquest here.\n",
    "    \n",
    " \n",
    "    #### Delay for 1 seconds ####\n",
    "    time.sleep(1)\n",
    "        \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    " \n",
    " ## Start loop ##\n",
    "for url in urls:\n",
    "    print(\"Current date & time \" + time.strftime(\"%c\"))\n",
    "\n",
    "    #use our new function to process each url\n",
    "    page = get_page(url)\n",
    "             \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")\n",
    "    \n",
    "    time.sleep(.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Save as you go\n",
    "\n",
    "You might run into an issue halfway through your scrape and your script breaks. So you want to make sure you are saving your data as you go.  \n",
    "\n",
    "~~~ \n",
    "import csv\n",
    "...\n",
    "with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # collected_items = [\n",
    "    #   [\"Product #1\", \"10\", \"http://example.com/product-1\"],\n",
    "    #   [\"Product #2\", \"25\", \"http://example.com/product-2\"],\n",
    "    #   ...\n",
    "    # ]\n",
    "\n",
    "    for item_property_list in collected_items:\n",
    "        writer.writerow(item_property_list)\n",
    "~~~\n",
    "~~~\n",
    "import csv\n",
    "...\n",
    "field_names = [\"Product Name\", \"Price\", \"Detail URL\"]\n",
    "with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, field_names)\n",
    "\n",
    "    # collected_items = [\n",
    "    #   {\n",
    "    #       \"Product Name\": \"Product #1\",\n",
    "    #       \"Price\": \"10\",\n",
    "    #       \"Detail URL\": \"http://example.com/product-1\"\n",
    "    #   },\n",
    "    #   ...\n",
    "    # ]\n",
    "\n",
    "    # Write a header row\n",
    "    writer.writerow({x: x for x in field_names})\n",
    "\n",
    "    for item_property_dict in collected_items:\n",
    "        writer.writerow(item_property_dict)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    print(\"Current date & time \" + time.strftime(\"%c\"))\n",
    "\n",
    "    #use our new function to process each url\n",
    "    page = get_page(url)\n",
    "             \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")\n",
    "    \n",
    "    #Saving your data as you go\n",
    "    \n",
    "    # Option 1: write the line of data to a csv files\n",
    "    with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "    for item in items:\n",
    "        writer.writerow(item)\n",
    "        \n",
    "    # Option 2: Inseting the data into a DB\n",
    "    # This code uses a theoretical module, SQL,\n",
    "    # The functions below are examples and will not run. \n",
    "    import sql_helpers as sql\n",
    "    \n",
    "    sql.create_connection()\n",
    "    for  item in items:\n",
    "        item = data\n",
    "        query = \"INSERT INTO table_name VALUES (%s,%s,%s,%s)\"\n",
    "        sql.insert_data(db, query, data )\n",
    "        \n",
    "    #Taking a one second pause to help slow down your requests \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources \n",
    "- [More advanced issues](https://blog.hartleybrody.com/web-scraping-cheat-sheet/)\n",
    "- [Request Advanced Usage](http://docs.python-requests.org/en/master/user/advanced/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping with Python often requires no more than the use of the Beautiful Soup module to reach the goal. Beautiful Soup is a popular Python library that makes web scraping by traversing the DOM (document object model) easier to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/beautiful-soup-web-scraper-python/#part-3-parse-html-code-with-beautiful-soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied: Scraping Amazon's Best Sellers list:\n",
    "\n",
    "\n",
    "Amazon keeps track of the best sellers for 41 different categories of products. We want to grab that data from Amazon so that we can keep track of which products are on that list and stock our mom and pop store with them.  \n",
    "\n",
    "\n",
    "Deliverable: a file that contains all of the products on Amazon's best seller list. \n",
    "\n",
    "```[{'name': 'A top selling product',\n",
    "'url': http://the_url_to_the_product.com},\n",
    "{'name': 'A top selling product',\n",
    "'url': http://the_url_to_the_product.com}]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start by grabbing the page where all of the best sellers list are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url=\"https://www.amazon.com/Best-Sellers/zgbs\"\n",
    "\n",
    "#let's use the function we already created\n",
    "page = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html dir=\"ltr\">\n",
      " <head>\n",
      "  <link href=\"https://images-na.ssl-images-amazon.com/images/I/21doGy6C0kL._RC|01KD4yyr5LL.css_.css?AUIClients/ZeitgeistPageAssets-zeitgeistHome\" rel=\"stylesheet\"/>\n",
      "  <link href=\"https://images-na.ssl-images-amazon.com/images/I/51tax7M48-L._RC|516fcOUE-HL.css,01evdoiemkL.css,01K+Ps1DeEL.css,31pdJv9iSzL.css,01VszOUTO6L.css,11UGC+GXOPL.css,21LK7jaicML.css,11L58Qpo0GL.css,21kyTi1FabL.css,01ruG+gDPFL.css,01YhS3Cs-hL.css,21GwE3cR-yL.css,019SHZnt8RL.css,01wAWQRgXzL.css,21bWcRJYNIL.css,11WgRxUdJRL.css,01dU8+SPlFL.css,11ocrgKoE-L.css,01SHjPML6tL.css,111-D2qRjiL.css,01QrWuRrZ-L.css,310Imb6LqFL.css,01piEq-AdwL.css,11Z1a0FxSIL.css,01cbS3UK11L.css,21mOLw+nYYL.css,01L8Y-JFEhL.css_.css?AUIClients/AmazonUI#us.not-trident.218320-T1\" rel=\"stylesheet\"/>\n",
      "  <script>\n",
      "   (function(f,h,R,A){function G(a){x&&x.tag&&x.tag(q(\":\",\"aui\",a))}function v(a,b){x&&x.count&&x.count(\"aui:\"+a,0===b?0:b||(x.count(\"aui:\"+a)||0)+1)}function p(a){try{return a.test(navigator.userAgent)}catch(b){return!1}}function y(a,b,c){a.addEventListener?a.addEventListener(b,c,!1):a.attachEvent&&a.attachEvent(\"on\"+b,c)}function q(a,b,c,e){b=b&&c?b+a+c:b||c;return e?q(a,b,e):b}function H(a,b,c){try{Object.defineProperty(a,b,{value:c,writable:!1})}catch(e){a[b]=c}return c}function ua(a,b){var c=a.length,\n",
      "e=c,g=function(){e--||(S.push(b),T||(setTimeout(U,0),T=!0))};for(g();c--;)da[a[c]]?g():(B[a[c]]=B[a[c]]||[]).push(g)}function va(a,b,c,e,g){var d=h.createElement(a?\"script\":\"link\");y(d,\"error\",e);g&&y(d,\"load\",g);a?(d.type=\"text/javascript\",d.async=!0,c&&/AUIClients|images[/]I/.test(b)&&d.setAttribute(\"crossorigin\",\"anonymous\"),d.src=b):(d.rel=\"stylesheet\",d.href=b);h.getElementsByTagName(\"head\")[0].appendChild(d)}function ea(a,b){function c(c,e){function g(){va(b,c,h,function(b){!I&&h?(h=!1,v(\"resource_retry\"),\n",
      "g()):(v(\"resource_error\"),a.log(\"Asset failed to load: \"+c,I?\"WARN\":A));b&&b.stopPropagation?b.stopPropagation():f.event&&(f.event.cancelBubble=!0)},e)}if(fa[c])return!1;fa[c]=!0;v(\"resource_count\");var h=!0;return!g()}if(b){var e=0,g=0;c.andConfirm=function(a,b){return c(a,function(){e++;b&&b.apply(this,arguments)})};c.confirm=function(){g++};c.getCsriCounters=function(){return{reqs:e,full:g}}}return c}function wa(a,b,c){for(var e={name:a,guard:function(c){return b.guardFatal(a,c)},logError:function(c,\n",
      "d,e){b.logError(c,d,e,a)}},g=[],d=0;d<c.length;d++)J.hasOwnProperty(c[d])&&(g[d]=V.hasOwnProperty(c[d])?V[c[d]](J[c[d]],e):J[c[d]]);return g}function C(a,b,c,e,g){return function(d,h){function n(){var a=null;e?a=h:\"function\"===typeof h&&(p.start=D(),a=h.apply(f,wa(d,k,l)),p.end=D());if(b){J[d]=a;a=d;for(da[a]=!0;(B[a]||[]).length;)B[a].shift()();delete B[a]}p.done=!0}var k=g||this;\"function\"===typeof d&&(h=d,d=A);b&&(d=d?d.replace(ga,\"\"):\"__NONAME__\",W.hasOwnProperty(d)&&k.error(q(\", reregistered by \",\n",
      "q(\" by \",d+\" already registered\",W[d]),k.attribution),d),W[d]=k.attribution);for(var l=[],m=0;m<a.length;m++)l[m]=a[m].replace(ga,\"\");var p=ha[d||\"anon\"+ ++xa]={depend:l,registered:D(),namespace:k.namespace};c?n():ua(l,k.guardFatal(d,n));return{decorate:function(a){V[d]=k.guardFatal(d,a)}}}}function ia(a){return function(){var b=Array.prototype.slice.call(arguments);return{execute:C(b,!1,a,!1,this),register:C(b,!0,a,!1,this)}}}function X(a,b){return function(c,e){e||(e=c,c=A);var g=this.attribution;\n",
      "return function(){z.push(b||{attribution:g,name:c,logLevel:a});var d=e.apply(this,arguments);z.pop();return d}}}function K(a,b){this.load={js:ea(this,!0),css:ea(this)};H(this,\"namespace\",b);H(this,\"attribution\",a)}function ja(){h.body?r.trigger(\"a-bodyBegin\"):setTimeout(ja,20)}function E(a,b){a.className=Y(a,b)+\" \"+b}function Y(a,b){return(\" \"+a.className+\" \").split(\" \"+b+\" \").join(\" \").replace(/^ | $/g,\"\")}function ka(a){try{return a()}catch(b){return!1}}function L(){if(M){var a={w:f.innerWidth||\n",
      "n.clientWidth,h:f.innerHeight||n.clientHeight};5<Math.abs(a.w-Z.w)||50<a.h-Z.h?(Z=a,N=4,(a=k.mobile||k.tablet?450<a.w&&a.w>a.h:1250<=a.w)?E(n,\"a-ws\"):n.className=Y(n,\"a-ws\")):0<N&&(N--,la=setTimeout(L,16))}}function ya(a){(M=a===A?!M:!!a)&&L()}function za(){return M}function u(a,b){return\"sw:\"+(b||\"\")+\":\"+a+\":\"}function ma(){na.forEach(function(a){G(a)})}function t(a){na.push(a)}function oa(a,b,c,e){if(c){b=p(/Chrome/i)&&!p(/Edge/i)&&!p(/OPR/i)&&!a.capabilities.isAmazonApp&&!p(new RegExp(aa+\"bwv\"+\n",
      "aa+\"b\"));var g=u(e,\"browser\"),d=u(e,\"prod_mshop\"),f=u(e,\"beta_mshop\");!a.capabilities.isAmazonApp&&c.browser&&b&&(t(g+\"supported\"),c.browser.action(g,e));!b&&c.browser&&t(g+\"unsupported\");c.prodMshop&&t(d+\"unsupported\");c.betaMshop&&t(f+\"unsupported\")}}\"use strict\";var O=R.now=R.now||function(){return+new R},D=function(a){return a&&a.now?a.now.bind(a):O}(f.performance),P=D(),l=f.AmazonUIPageJS||f.P;if(l&&l.when&&l.register){for(var P=[],m=h.currentScript;m;m=m.parentElement)m.id&&P.push(m.id);return l.log(\"A copy of P has already been loaded on this page.\",\n",
      "\"FATAL\",P.join(\" \"))}var x=f.ue;G();G(\"aui_build_date:3.19.8-2019-11-06\");var S=[],T=!1,U;U=function(){for(var a=setTimeout(U,0),b=O();S.length;)if(S.shift()(),50<O()-b)return;clearTimeout(a);T=!1};var da={},B={},fa={},I=!1;y(f,\"beforeunload\",function(){I=!0;setTimeout(function(){I=!1},1E4)});var ga=/^prv:/,W={},J={},V={},ha={},xa=0,aa=String.fromCharCode(92),F,z=[],pa=f.onerror;f.onerror=function(a,b,c,e,g){g&&\"object\"===typeof g||(g=Error(a,b,c),g.columnNumber=e,g.stack=b||c||e?q(aa,g.message,\"at \"+\n",
      "q(\":\",b,c,e)):A);var d=z.pop()||{};g.attribution=q(\":\",g.attribution||d.attribution,d.name);g.logLevel=d.logLevel;g.attribution&&console&&console.log&&console.log([g.logLevel||\"ERROR\",a,\"thrown by\",g.attribution].join(\" \"));z=[];pa&&(d=[].slice.call(arguments),d[4]=g,pa.apply(f,d))};K.prototype={logError:function(a,b,c,e){b={message:b,logLevel:c||\"ERROR\",attribution:q(\":\",this.attribution,e)};if(f.ueLogError)return f.ueLogError(a||b,a?b:null),!0;console&&console.error&&(console.log(b),console.error(a));\n",
      "return!1},error:function(a,b,c,e){a=Error(q(\":\",e,a,c));a.attribution=q(\":\",this.attribution,b);throw a;},guardError:X(),guardFatal:X(\"FATAL\"),guardCurrent:function(a){var b=z[z.length-1];return b?X(b.logLevel,b).call(this,a):a},log:function(a,b,c){return this.logError(null,a,b,c)},declare:C([],!0,!0,!0),register:C([],!0),execute:C([]),AUI_BUILD_DATE:\"3.19.8-2019-11-06\",when:ia(),now:ia(!0),trigger:function(a,b,c){var e=O();this.declare(a,{data:b,pageElapsedTime:e-(f.aPageStart||NaN),triggerTime:e});\n",
      "c&&c.instrument&&F.when(\"prv:a-logTrigger\").execute(function(b){b(a)})},handleTriggers:function(){this.log(\"handleTriggers deprecated\")},attributeErrors:function(a){return new K(a)},_namespace:function(a,b){return new K(a,b)}};var r=H(f,\"AmazonUIPageJS\",new K);F=r._namespace(\"PageJS\",\"AmazonUI\");F.declare(\"prv:p-debug\",ha);r.declare(\"p-recorder-events\",[]);r.declare(\"p-recorder-stop\",function(){});H(f,\"P\",r);ja();if(h.addEventListener){var qa;h.addEventListener(\"DOMContentLoaded\",qa=function(){r.trigger(\"a-domready\");\n",
      "h.removeEventListener(\"DOMContentLoaded\",qa,!1)},!1)}var n=h.documentElement,ba=function(){var a=[\"O\",\"ms\",\"Moz\",\"Webkit\"],b=h.createElement(\"div\");return{testGradients:function(){b.style.cssText=\"background-image:-webkit-gradient(linear,left top,right bottom,from(#1E4),to(white));background-image:-webkit-linear-gradient(left top,#1E4,white);background-image:linear-gradient(left top,#1E4,white);\";return~b.style.backgroundImage.indexOf(\"gradient\")},test:function(c){var e=c.charAt(0).toUpperCase()+\n",
      "c.substr(1);c=(a.join(e+\" \")+e+\" \"+c).split(\" \");for(e=c.length;e--;)if(\"\"===b.style[c[e]])return!0;return!1},testTransform3d:function(){var a=!1;f.matchMedia&&(a=f.matchMedia(\"(-webkit-transform-3d)\").matches);return a}}}(),l=n.className,ra=/(^| )a-mobile( |$)/.test(l),sa=/(^| )a-tablet( |$)/.test(l),k={audio:function(){return!!h.createElement(\"audio\").canPlayType},video:function(){return!!h.createElement(\"video\").canPlayType},canvas:function(){return!!h.createElement(\"canvas\").getContext},svg:function(){return!!h.createElementNS&&\n",
      "!!h.createElementNS(\"http://www.w3.org/2000/svg\",\"svg\").createSVGRect},offline:function(){return navigator.hasOwnProperty&&navigator.hasOwnProperty(\"onLine\")&&navigator.onLine},dragDrop:function(){return\"draggable\"in h.createElement(\"span\")},geolocation:function(){return!!navigator.geolocation},history:function(){return!(!f.history||!f.history.pushState)},webworker:function(){return!!f.Worker},autofocus:function(){return\"autofocus\"in h.createElement(\"input\")},inputPlaceholder:function(){return\"placeholder\"in\n",
      "h.createElement(\"input\")},textareaPlaceholder:function(){return\"placeholder\"in h.createElement(\"textarea\")},localStorage:function(){return\"localStorage\"in f&&null!==f.localStorage},orientation:function(){return\"orientation\"in f},touch:function(){return\"ontouchend\"in h},gradients:function(){return ba.testGradients()},hires:function(){var a=f.devicePixelRatio&&1.5<=f.devicePixelRatio||f.matchMedia&&f.matchMedia(\"(min-resolution:144dpi)\").matches;v(\"hiRes\"+(ra?\"Mobile\":sa?\"Tablet\":\"Desktop\"),a?1:0);\n",
      "return a},transform3d:function(){return ba.testTransform3d()},touchScrolling:function(){return p(/Windowshop|android|OS ([5-9]|[1-9][0-9]+)(_[0-9]{1,2})+ like Mac OS X|Chrome|Silk|Firefox|Trident.+?; Touch/i)},ios:function(){return p(/OS [1-9][0-9]*(_[0-9]*)+ like Mac OS X/i)&&!p(/trident|Edge/i)},android:function(){return p(/android.([1-9]|[L-Z])/i)&&!p(/trident|Edge/i)},mobile:function(){return ra},tablet:function(){return sa},rtl:function(){return\"rtl\"===n.dir}};for(m in k)k.hasOwnProperty(m)&&\n",
      "(k[m]=ka(k[m]));for(var ca=\"textShadow textStroke boxShadow borderRadius borderImage opacity transform transition\".split(\" \"),Q=0;Q<ca.length;Q++)k[ca[Q]]=ka(function(){return ba.test(ca[Q])});var M=!0,la=0,Z={w:0,h:0},N=4;L();y(f,\"resize\",function(){clearTimeout(la);N=4;L()});var ta={getItem:function(a){try{return f.localStorage.getItem(a)}catch(b){}},setItem:function(a,b){try{return f.localStorage.setItem(a,b)}catch(c){}}};n.className=Y(n,\"a-no-js\");E(n,\"a-js\");!p(/OS [1-8](_[0-9]*)+ like Mac OS X/i)||\n",
      "f.navigator.standalone||p(/safari/i)||E(n,\"a-ember\");l=[];for(m in k)k.hasOwnProperty(m)&&k[m]&&l.push(\"a-\"+m.replace(/([A-Z])/g,function(a){return\"-\"+a.toLowerCase()}));E(n,l.join(\" \"));n.setAttribute(\"data-aui-build-date\",\"3.19.8-2019-11-06\");r.register(\"p-detect\",function(){return{capabilities:k,localStorage:k.localStorage&&ta,toggleResponsiveGrid:ya,responsiveGridEnabled:za}});p(/UCBrowser/i)||k.localStorage&&E(n,ta.getItem(\"a-font-class\"));r.declare(\"a-event-revised-handling\",!1);var w;try{w=\n",
      "navigator.serviceWorker}catch(a){G(\"sw:nav_err\")}w&&(y(w,\"message\",function(a){a&&a.data&&v(a.data.k,a.data.v)}),w.controller&&w.controller.postMessage(\"MSG-RDY\"));var na=[],l={reg:{},unreg:{}};l.unreg.browser={action:function(a,b){var c=w.getRegistrations();c&&c.then(function(c){c.forEach(function(c){c.unregister().then(function(){v(a+\"success\")}).catch(function(c){r.logError(c,\"[AUI SW] Failed to \"+b+\" service worker: \");v(a+\"failure\")})})})}};(function(a){var b=a.reg,c=a.unreg;w&&w.getRegistrations?\n",
      "(F.when(\"A\",\"a-util\").execute(function(a,b){oa(a,b,c,\"unregister\")}),y(f,\"load\",function(){F.when(\"A\",\"a-util\").execute(function(a,c){oa(a,c,b,\"register\");ma()})})):(b&&(b.browser&&t(u(\"register\",\"browser\")+\"unsupported\"),b.prodMshop&&t(u(\"register\",\"prod_mshop\")+\"unsupported\"),b.betaMshop&&t(u(\"register\",\"beta_mshop\")+\"unsupported\")),c&&(c.browser&&t(u(\"unregister\",\"browser\")+\"unsupported\"),c.prodMshop&&t(u(\"unregister\",\"prod_mshop\")+\"unsupported\"),c.betaMshop&&t(u(\"unregister\",\"beta_mshop\")+\"unsupported\")),\n",
      "ma())})(l);r.declare(\"a-fix-event-off\",!1);v(\"pagejs:pkgExecTime\",D()-P)})(window,document,Date);\n",
      "  (window.AmazonUIPageJS ? AmazonUIPageJS : P).load.js('https://images-na.ssl-images-amazon.com/images/I/01JMC-kSjXL.js?AUIClients/ZeitgeistPageAssets-zeitgeistHome');\n",
      "  (window.AmazonUIPageJS ? AmazonUIPageJS : P).load.js('https://images-na.ssl-images-amazon.com/images/I/61-6nKPKyWL._RC|11-BZEJ8lnL.js,61GQ9IdK7HL.js,21Of0-9HPCL.js,012FVc3131L.js,119KAWlHU6L.js,51CF7BmbF2L.js,11AHlQhPRjL.js,016iHgpF74L.js,11aNYFFS5hL.js,116tgw9TSaL.js,211-p4GRUCL.js,01PoLXBDXWL.js,61VSCV4uJuL.js,01ezj5Rkz1L.js,11BOgvnnntL.js,31UWuPgtTtL.js,01rpauTep4L.js,01iyxuSGj4L.js,01OTNfCf5oL.js_.js?AUIClients/AmazonUI#218320-T1.191015-T1');\n",
      "  </script>\n",
      "  <title>\n",
      "   Amazon.com Best Sellers: The most popular items on Amazon\n",
      "  </title>\n",
      "  <meta content=\"Discover the best  in Best Sellers.  Find the top 100 most popular items in Amazon  Best Sellers.\" name=\"description\"/>\n",
      "  <link href=\"https://www.amazon.com/Best-Sellers/zgbs\" rel=\"canonical\"/>\n",
      "  <link href=\"https://images-na.ssl-images-amazon.com/images/G/01/zeitgeist/static/css/zeitgeist-home.4._CB526916344_.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <!-- BeginNav -->\n",
      "  <script type=\"text/javascript\">\n",
      "   var nav_t_begin_nav = + new Date();\n",
      "  </script>\n",
      "  <!-- From remote config -->\n",
      "  <style type=\"text/css\">\n",
      "   .nav-sprite-v1 .nav-sprite, .nav-sprite-v1 .nav-icon {\n",
      "  background-image: url(https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/nav-sprite-global_bluebeacon-1x_optimized_layout1._CB468670774_.png);\n",
      "  background-position: 0 1000px;\n",
      "  background-repeat: repeat-x;\n",
      "}\n",
      ".nav-spinner {\n",
      "  background-image: url(https://images-na.ssl-images-amazon.com/images/G/01/javascripts/lib/popover/images/snake._CB192571611_.gif);\n",
      "  background-position: center center;\n",
      "  background-repeat: no-repeat;\n",
      "}\n",
      ".nav-timeline-icon, .nav-access-image, .nav-timeline-prime-icon {\n",
      "  background-image: url(https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/timeline_sprite_1x._CB276239408_.png);\n",
      "  background-repeat: no-repeat;\n",
      "}\n",
      "  </style>\n",
      "  <script type=\"text/javascript\">\n",
      "   var nav_t_after_inline_CSS = + new Date();\n",
      "  </script>\n",
      "  <!-- NAVYAAN CSS -->\n",
      "  <link href=\"https://images-na.ssl-images-amazon.com/images/I/21rQMjhzuzL._RC|71KwxxRxz9L.css,11-cFHXC3yL.css,31DAr4NkZQL.css,21lRUdwotiL.css,41tc24mJIGL.css,11G4HxMtMSL.css,31OvHRW+XiL.css,01XHMOHpK1L.css_.css?AUIClients/AmazonNavigationDesktopMetaAsset#desktop.222470-T1\" rel=\"stylesheet\"/>\n",
      "  <!-- NAVYAAN JS -->\n",
      "  <script>\n",
      "   (window.AmazonUIPageJS ? AmazonUIPageJS : P).when('navCF').execute(function() {\n",
      "  (window.AmazonUIPageJS ? AmazonUIPageJS : P).load.js('https://images-na.ssl-images-amazon.com/images/I/41G3rHj35aL._RC|715tld8dOIL.js,61J7BJn2HyL.js,41W9ohA0e+L.js,11vrNkbdcvL.js,21qaguVEGfL.js,31S40+p9LcL.js,51YeRc8UK1L.js,313jWehHlpL.js_.js?AUIClients/AmazonNavigationDesktopMetaAsset#desktop');\n",
      "});\n",
      "  </script>\n",
      "  <!-- From remote config v3-->\n",
      "  <script type=\"text/javascript\">\n",
      "   (function(d){document.createElement(\"header\");function b(e){return[].slice.call(e)}function c(f,e){return{m:f,a:b(e)}}var a=function(f){var g={};g._sourceName=f;g._replay=[];g.getNow=function(i,h){return h};function e(i,h,j){i[j]=function(){g._replay.push(h.concat(c(j,arguments)))}}g.when=function(){var i=[c(\"when\",arguments)];var h={};e(h,i,\"run\");e(h,i,\"declare\");e(h,i,\"publish\");e(h,i,\"build\");return h};e(g,[],\"declare\");e(g,[],\"build\");e(g,[],\"publish\");e(g,[],\"importEvent\");a._shims.push(g);return g};a._shims=[];if(!d.$Nav){d.$Nav=a(\"rcx-nav\")}if(!d.$Nav.make){d.$Nav.make=a}}(window));\n",
      "$Nav.importEvent('navbarJS-beaconbelt');\n",
      "$Nav.declare('img.sprite', {\n",
      "  'png8': 'https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/global-sprite_bluebeacon-v1._CB327533540_.png',\n",
      "  'png32': 'https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/nav-sprite-global_bluebeacon-1x_optimized_layout1._CB468670774_.png',\n",
      "  'png32-2x': 'https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/nav-sprite-global_bluebeacon-2x_optimized_layout1._CB468670774_.png'\n",
      "});\n",
      "$Nav.declare('img.timeline', {\n",
      "  'timeline-icon-2x': 'https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/timeline_sprite_2x._CB276239408_.png'\n",
      "});\n",
      "window._navbarSpriteUrl = 'https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/nav-sprite-global_bluebeacon-1x_optimized_layout1._CB468670774_.png';\n",
      "$Nav.declare('img.pixel', 'https://images-na.ssl-images-amazon.com/images/G/01/x-locale/common/transparent-pixel._CB386942464_.gif');\n",
      "  </script>\n",
      "  <img alt=\"\" src=\"https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/nav-sprite-global_bluebeacon-1x_optimized_layout1._CB468670774_.png\" style=\"display:none\"/>\n",
      "  <!--[if IE 6]>\n",
      "<style type=\"text/css\"><!--\n",
      "  #navbar.nav-sprite-v3 .nav-sprite {\n",
      "    background-image: url(https://images-na.ssl-images-amazon.com/images/G/01/gno/sprites/global-sprite_bluebeacon-v1._CB327533540_.png);\n",
      "  }\n",
      "-->\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this page, we want to find the urls of all the other pages to scrape those.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = soup.find('ul', {id:'zg_browseRoot'})\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the select statement to find the elements containing each url\n",
    "urls = soup.find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using the select statement to find the elements containing each url\n",
    "urls = soup.select('ul#zg_browseRoot a' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    print(url['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(urls[4].text, '\\n',urls[4]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all best seller urls\n",
    "urls = [url['href'] for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a url/products that you want to investigate and lets build our script to parse one page.  then we can apply it to all of the pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing the products from each page. \n",
    "\n",
    "Now that we have the URL for the pages, we want to parse those to get the actual information about the bestselling products.\n",
    "\n",
    "So now we need to go over each best seller link and parse that page to grabe the prodcuts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing one specific URL to parse\n",
    "url=urls[3]\n",
    "\n",
    "apps = requests.get(url)\n",
    "apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_soup = BS(apps.content, 'html.parser')\n",
    "print(app_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have grabbed one specifc dbest seller URL, but now we need to figure out how to grab the details about the product. \n",
    "\n",
    "- Inspect the actual webpage to determine the data you want and the corresponding elements you want to parse out. \n",
    "\n",
    "- Then use that element tag or class to pull those elements out of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = app_soup.select('.aok-inline-block.zg-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_soup.find_all(True, {'class':['aok-inline-block', 'zg-item']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "containers = app_soup.find_all(class_='aok-inline-block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in blocks[0].children:\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the data\n",
    "Now that you can access all the blocks for each prodcut, we need to pull out specific information for the products. \n",
    "\n",
    "- Think about what data you need from the 'block' and create a sample data stucture that you will want to use.  \n",
    "\n",
    "- Parse one block into that data structure.\n",
    "\n",
    "- Put this into a loop so that we can proccess all of the products and create one list with all of the data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have each individual part working, let's wrap this all up in a function that we can run for each product class.\n",
    "\n",
    "Create a function that takes in a URL(product category)\n",
    "\n",
    "This function should use the code above that parses the individual products on the page.\n",
    "\n",
    "The funciton should then return all of that data in the correct format ( a list of dictionaries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bestseller_cat(___):\n",
    "    #your code here\n",
    "    \n",
    "    return ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is now to add this function to the larger script we have from above that will loop over our list of urls (categories) and grab all of the data we need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
